{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from analogainas.search_spaces.config_space import ConfigSpace\n",
    "## Default Search Space\n",
    "# We will be using the default search space\n",
    "CS = ConfigSpace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['out_channel0', 'M', 'R1', 'R2', 'R3', 'R4', 'R5', 'convblock1', 'widenfact1', 'B1', 'convblock2', 'widenfact2', 'B2', 'convblock3', 'widenfact3', 'B3', 'convblock4', 'widenfact4', 'B4', 'convblock5', 'widenfact5', 'B5']\n"
     ]
    }
   ],
   "source": [
    "# You can extract the full list of hyperparameters using:\n",
    "CS.get_hyperparameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "773094113280"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute the size of the search space\n",
    "CS.compute_cs_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 'results' directory already exists.\n",
      "\n",
      "Search 0 started\n",
      "2\n",
      "ITERATION 0 completed: best acc [0.69118345]\n",
      "ITERATION 1 completed: best acc [0.7981428]\n",
      "ITERATION 2 completed: best acc [0.7981428]\n",
      "ITERATION 3 completed: best acc [0.7981428]\n",
      "ITERATION 4 completed: best acc [0.7981428]\n",
      "ITERATION 5 completed: best acc [0.8489061]\n",
      "ITERATION 6 completed: best acc [0.8489061]\n",
      "ITERATION 7 completed: best acc [0.8489061]\n",
      "ITERATION 8 completed: best acc [0.8489061]\n",
      "ITERATION 9 completed: best acc [0.8489061]\n",
      "Best Acc = [0.8489061]\n",
      "Search 1 started\n",
      "2\n",
      "ITERATION 0 completed: best acc [0.8960939]\n",
      "ITERATION 1 completed: best acc [0.9320831]\n",
      "ITERATION 2 completed: best acc [0.9320831]\n",
      "ITERATION 3 completed: best acc [0.9320831]\n",
      "ITERATION 4 completed: best acc [0.9320831]\n",
      "ITERATION 5 completed: best acc [0.9320831]\n",
      "ITERATION 6 completed: best acc [0.9320831]\n",
      "ITERATION 7 completed: best acc [0.9320831]\n",
      "ITERATION 8 completed: best acc [0.9320831]\n",
      "ITERATION 9 completed: best acc [0.94062006]\n",
      "Best Acc = [0.94062006]\n",
      "Search 2 started\n",
      "2\n",
      "ITERATION 0 completed: best acc [0.93138456]\n",
      "ITERATION 1 completed: best acc [0.93138456]\n",
      "ITERATION 2 completed: best acc [0.93138456]\n",
      "ITERATION 3 completed: best acc [0.93138456]\n",
      "ITERATION 4 completed: best acc [0.93138456]\n",
      "ITERATION 5 completed: best acc [0.93138456]\n",
      "ITERATION 6 completed: best acc [0.93138456]\n",
      "ITERATION 7 completed: best acc [0.93138456]\n",
      "ITERATION 8 completed: best acc [0.93138456]\n",
      "ITERATION 9 completed: best acc [0.93138456]\n",
      "Best Acc = [0.93138456]\n",
      "Search 3 started\n",
      "2\n",
      "ITERATION 0 completed: best acc [0.75776285]\n",
      "ITERATION 1 completed: best acc [0.7978291]\n",
      "ITERATION 2 completed: best acc [0.7978291]\n",
      "ITERATION 3 completed: best acc [0.7978291]\n",
      "ITERATION 4 completed: best acc [0.7978291]\n",
      "ITERATION 5 completed: best acc [0.7978291]\n",
      "ITERATION 6 completed: best acc [0.7978291]\n",
      "ITERATION 7 completed: best acc [0.7978291]\n",
      "ITERATION 8 completed: best acc [0.7978291]\n",
      "ITERATION 9 completed: best acc [0.94490695]\n",
      "Best Acc = [0.94490695]\n",
      "Search 4 started\n",
      "2\n",
      "ITERATION 0 completed: best acc [0.910607]\n",
      "ITERATION 1 completed: best acc [0.910607]\n",
      "ITERATION 2 completed: best acc [0.95465887]\n",
      "ITERATION 3 completed: best acc [0.95465887]\n",
      "ITERATION 4 completed: best acc [0.95465887]\n",
      "ITERATION 5 completed: best acc [0.95465887]\n",
      "ITERATION 6 completed: best acc [0.95465887]\n",
      "ITERATION 7 completed: best acc [0.95465887]\n",
      "ITERATION 8 completed: best acc [0.95465887]\n",
      "ITERATION 9 completed: best acc [0.95465887]\n",
      "Best Acc = [0.95465887]\n",
      "SEARCH ENDED\n"
     ]
    }
   ],
   "source": [
    "# Load the evaluator\n",
    "from analogainas.evaluators.xgboost import XGBoostEvaluator\n",
    "evaluator = XGBoostEvaluator()\n",
    "from analogainas.search_algorithms.ea_optimized import EAOptimizer\n",
    "from analogainas.search_algorithms.worker import Worker\n",
    "NB_RUN = 5\n",
    "\n",
    "optimizer = EAOptimizer(evaluator, population_size=20, nb_iter=10)\n",
    "worker = Worker(CS, optimizer=optimizer, runs=NB_RUN)\n",
    "worker.search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best architecture accuracy:  [0.95465887]\n",
      "Standard deviation of accuracy over 5 runs: 0.019166210841573973\n",
      "Best architecture:  {'out_channel0': 12, 'M': 4, 'R1': 3, 'R2': 13, 'R3': 14, 'R4': 7, 'R5': 0, 'convblock1': 2, 'widenfact1': 0.7929658310097608, 'B1': 4, 'convblock2': 2, 'widenfact2': 0.6257704362802659, 'B2': 1, 'convblock3': 1, 'widenfact3': 0.6472810569679449, 'B3': 4, 'convblock4': 2, 'widenfact4': 0.6230044356013043, 'B4': 3, 'convblock5': 0, 'widenfact5': 0, 'B5': 0}\n"
     ]
    }
   ],
   "source": [
    "worker.result_summary()\n",
    "best_config = worker.best_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'out_channel0': 12,\n",
       " 'M': 4,\n",
       " 'R1': 3,\n",
       " 'R2': 13,\n",
       " 'R3': 14,\n",
       " 'R4': 7,\n",
       " 'R5': 0,\n",
       " 'convblock1': 2,\n",
       " 'widenfact1': 0.7929658310097608,\n",
       " 'B1': 4,\n",
       " 'convblock2': 2,\n",
       " 'widenfact2': 0.6257704362802659,\n",
       " 'B2': 1,\n",
       " 'convblock3': 1,\n",
       " 'widenfact3': 0.6472810569679449,\n",
       " 'B3': 4,\n",
       " 'convblock4': 2,\n",
       " 'widenfact4': 0.6230044356013043,\n",
       " 'B4': 3,\n",
       " 'convblock5': 0,\n",
       " 'widenfact5': 0,\n",
       " 'B5': 0}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [1, 1, 3, 96, 96]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01manalogainas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Building the Optimal Sub-network\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mNetwork\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbest_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Masters/NYU_Courses/Spring_2024/High Performance Machine Learning/Project/Supernet_Project/Final_Project/analog-nas-unet/analogainas/search_spaces/resnet_macro_architecture.py:276\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m(self, config, input_dim, output_dim)\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mM \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m    271\u001b[0m     feature_maps_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\n\u001b[1;32m    272\u001b[0m         \u001b[38;5;28mround\u001b[39m(feature_maps_in \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwiden_factors[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGroup_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(m)])\n\u001b[1;32m    273\u001b[0m     )\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks\u001b[38;5;241m.\u001b[39madd_module(\n\u001b[1;32m    275\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGroup_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(m),\n\u001b[0;32m--> 276\u001b[0m         ResidualGroup(\n\u001b[1;32m    277\u001b[0m             block,\n\u001b[1;32m    278\u001b[0m             feature_maps_in,\n\u001b[1;32m    279\u001b[0m             feature_maps_out,\n\u001b[1;32m    280\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresidual_blocks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGroup_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(m)],\n\u001b[1;32m    281\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilters_size[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGroup_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(m)],\n\u001b[1;32m    282\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mres_branches[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGroup_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(m)],\n\u001b[1;32m    283\u001b[0m             \u001b[38;5;241m2\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mM, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mM \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    284\u001b[0m         ),\n\u001b[1;32m    285\u001b[0m     )\n\u001b[1;32m    286\u001b[0m     feature_maps_in \u001b[38;5;241m=\u001b[39m feature_maps_out\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_maps_out \u001b[38;5;241m=\u001b[39m feature_maps_out\n",
      "File \u001b[0;32m/opt/anaconda3/envs/supernetwork_nas/lib/python3.9/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/supernetwork_nas/lib/python3.9/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/supernetwork_nas/lib/python3.9/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/supernetwork_nas/lib/python3.9/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/supernetwork_nas/lib/python3.9/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/supernetwork_nas/lib/python3.9/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/supernetwork_nas/lib/python3.9/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [1, 1, 3, 96, 96]"
     ]
    }
   ],
   "source": [
    "from analogainas.search_spaces.resnet_macro_architecture import Network\n",
    "from analogainas.search_spaces.train import train_config_unet\n",
    "from analogainas.utils import *\n",
    "\n",
    "# Building the Optimal Sub-network\n",
    "model = Network(best_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metric Functions for Training Nuclei Segmentation\n",
    "\n",
    "# IOU Score and DICE Coefficients\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def iou_score(output, target):\n",
    "    smooth = 1e-5\n",
    "\n",
    "    if torch.is_tensor(output):\n",
    "        output = torch.sigmoid(output).data.cpu().numpy()\n",
    "    if torch.is_tensor(target):\n",
    "        target = target.data.cpu().numpy()\n",
    "    output_ = output > 0.5\n",
    "    target_ = target > 0.5\n",
    "    intersection = (output_ & target_).sum()\n",
    "    union = (output_ | target_).sum()\n",
    "\n",
    "    return (intersection + smooth) / (union + smooth)\n",
    "\n",
    "\n",
    "def dice_coef(output, target):\n",
    "    smooth = 1e-5\n",
    "\n",
    "    output = torch.sigmoid(output).view(-1).data.cpu().numpy()\n",
    "    target = target.view(-1).data.cpu().numpy()\n",
    "    intersection = (output * target).sum()\n",
    "\n",
    "    return (2. * intersection + smooth) / \\\n",
    "        (output.sum() + target.sum() + smooth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "\n",
    "import argparse\n",
    "def str2bool(v):\n",
    "    if v.lower() in ['true', 1]:\n",
    "        return True\n",
    "    elif v.lower() in ['false', 0]:\n",
    "        return False\n",
    "    else:\n",
    "        raise argparse.ArgumentTypeError('Boolean value expected.')\n",
    "\n",
    "\n",
    "def count_params(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Functions for Segmentation\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BCEDiceLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        bce = F.binary_cross_entropy_with_logits(input, target)\n",
    "        smooth = 1e-5\n",
    "        input = torch.sigmoid(input)\n",
    "        num = target.size(0)\n",
    "        input = input.view(num, -1)\n",
    "        target = target.view(num, -1)\n",
    "        intersection = (input * target)\n",
    "        dice = (2. * intersection.sum(1) + smooth) / (input.sum(1) + target.sum(1) + smooth)\n",
    "        dice = 1 - dice.sum() / num\n",
    "        return 0.5 * bce + dice\n",
    "\n",
    "\n",
    "class LovaszHingeLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        input = input.squeeze(1)\n",
    "        target = target.squeeze(1)\n",
    "        loss = lovasz_hinge(input, target, per_image=True)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "        'spatial_dims': 2,  # it's 2 for 2D images; would be 3 for 3D images\n",
    "        'in_channels': 3,   # e.g., 3 for RGB input images\n",
    "        'out_channels': 1,  # e.g., 1 for binary segmentation tasks\n",
    "        'channels': (16, 32, 64, 128, 256), # Number of channels in the inner layers\n",
    "        'strides': (2, 2, 2, 2),\n",
    "        'name': None,\n",
    "        'epochs': 200,\n",
    "        'batch_size': 64,\n",
    "        'arch': 'UNet',\n",
    "        'deep_supervision': False,\n",
    "        'input_channels': 3,\n",
    "        'num_classes': 1,\n",
    "        'input_w': 96,\n",
    "        'input_h': 96,\n",
    "        'loss': 'BCEDiceLoss',\n",
    "        'dataset': 'dsb2018_96',\n",
    "        'img_ext': '.png',\n",
    "        'mask_ext': '.png',\n",
    "        'optimizer': 'SGD',\n",
    "        'lr': 1e-3,\n",
    "        'momentum': 0.9,\n",
    "        'weight_decay': 1e-4,\n",
    "        'nesterov': False,\n",
    "        'scheduler': 'CosineAnnealingLR',\n",
    "        'min_lr': 1e-5,\n",
    "        'factor': 0.1,\n",
    "        'patience': 2,\n",
    "        'milestones': '1,2',\n",
    "        'gamma': 2/3,\n",
    "        'early_stopping': -1,\n",
    "        'num_workers': 4\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Digital Training imports\n",
    "import argparse\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "from glob import glob\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import yaml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim import lr_scheduler\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOSS_NAMES = ['BCEDiceLoss', 'LovaszHingeLoss']\n",
    "LOSS_NAMES.append('BCEWithLogitsLoss')\n",
    "\n",
    "def train(config, train_loader, model, criterion, optimizer):\n",
    "    avg_meters = {'loss': AverageMeter(),\n",
    "                  'iou': AverageMeter()}\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    pbar = tqdm(total=len(train_loader))\n",
    "    for input, target, _ in train_loader:\n",
    "        input = input.cuda()\n",
    "        target = target.cuda()\n",
    "\n",
    "        # compute output\n",
    "        if config['deep_supervision']:\n",
    "            outputs = model(input)\n",
    "            loss = 0\n",
    "            for output in outputs:\n",
    "                loss += criterion(output, target)\n",
    "            loss /= len(outputs)\n",
    "            iou = iou_score(outputs[-1], target)\n",
    "        else:\n",
    "            output = model(input)\n",
    "            loss = criterion(output, target)\n",
    "            iou = iou_score(output, target)\n",
    "\n",
    "        # compute gradient and do optimizing step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        avg_meters['loss'].update(loss.item(), input.size(0))\n",
    "        avg_meters['iou'].update(iou, input.size(0))\n",
    "\n",
    "        postfix = OrderedDict([\n",
    "            ('loss', avg_meters['loss'].avg),\n",
    "            ('iou', avg_meters['iou'].avg),\n",
    "        ])\n",
    "        pbar.set_postfix(postfix)\n",
    "        pbar.update(1)\n",
    "    pbar.close()\n",
    "\n",
    "    return OrderedDict([('loss', avg_meters['loss'].avg),\n",
    "                        ('iou', avg_meters['iou'].avg)])\n",
    "\n",
    "def validate(config, val_loader, model, criterion):\n",
    "    avg_meters = {'loss': AverageMeter(),\n",
    "                  'iou': AverageMeter()}\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(total=len(val_loader))\n",
    "        for input, target, _ in val_loader:\n",
    "            input = input.cuda()\n",
    "            target = target.cuda()\n",
    "\n",
    "            # compute output\n",
    "            if config['deep_supervision']:\n",
    "                outputs = model(input)\n",
    "                loss = 0\n",
    "                for output in outputs:\n",
    "                    loss += criterion(output, target)\n",
    "                loss /= len(outputs)\n",
    "                iou = iou_score(outputs[-1], target)\n",
    "            else:\n",
    "                output = model(input)\n",
    "                loss = criterion(output, target)\n",
    "                iou = iou_score(output, target)\n",
    "\n",
    "            avg_meters['loss'].update(loss.item(), input.size(0))\n",
    "            avg_meters['iou'].update(iou, input.size(0))\n",
    "\n",
    "            postfix = OrderedDict([\n",
    "                ('loss', avg_meters['loss'].avg),\n",
    "                ('iou', avg_meters['iou'].avg),\n",
    "            ])\n",
    "            pbar.set_postfix(postfix)\n",
    "            pbar.update(1)\n",
    "        pbar.close()\n",
    "\n",
    "    return OrderedDict([('loss', avg_meters['loss'].avg),\n",
    "                        ('iou', avg_meters['iou'].avg)])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'parameters'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 13\u001b[0m\n\u001b[1;32m      9\u001b[0m cudnn\u001b[38;5;241m.\u001b[39mbenchmark \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     11\u001b[0m model \u001b[38;5;241m=\u001b[39m {} \u001b[38;5;66;03m# Use your NAS architecture here\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m p: p\u001b[38;5;241m.\u001b[39mrequires_grad, \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m())\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moptimizer\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAdam\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     16\u001b[0m     optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(\n\u001b[1;32m     17\u001b[0m         params, lr\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m], weight_decay\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweight_decay\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'parameters'"
     ]
    }
   ],
   "source": [
    "# define loss function (criterion)\n",
    "if config['loss'] == 'BCEWithLogitsLoss':\n",
    "    criterion = nn.BCEWithLogitsLoss().cuda()\n",
    "elif config['loss'] == 'BCEDiceLoss':\n",
    "    criterion = BCEDiceLoss().cuda()\n",
    "else:\n",
    "    criterion = LovaszHingeLoss().cuda()\n",
    "\n",
    "cudnn.benchmark = True\n",
    "\n",
    "model = {} # Use your NAS architecture here\n",
    "\n",
    "params = filter(lambda p: p.requires_grad, model.parameters())\n",
    "\n",
    "if config['optimizer'] == 'Adam':\n",
    "    optimizer = optim.Adam(\n",
    "        params, lr=config['lr'], weight_decay=config['weight_decay'])\n",
    "elif config['optimizer'] == 'SGD':\n",
    "    optimizer = optim.SGD(params, lr=config['lr'], momentum=config['momentum'],\n",
    "                          nesterov=config['nesterov'], weight_decay=config['weight_decay'])\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "\n",
    "if config['scheduler'] == 'CosineAnnealingLR':\n",
    "    scheduler = lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer, T_max=config['epochs'], eta_min=config['min_lr'])\n",
    "elif config['scheduler'] == 'ReduceLROnPlateau':\n",
    "    scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, factor=config['factor'], patience=config['patience'],\n",
    "                                                verbose=1, min_lr=config['min_lr'])\n",
    "elif config['scheduler'] == 'MultiStepLR':\n",
    "    scheduler = lr_scheduler.MultiStepLR(optimizer, milestones=[int(e) for e in config['milestones'].split(',')], gamma=config['gamma'])\n",
    "elif config['scheduler'] == 'ConstantLR':\n",
    "    scheduler = None\n",
    "else:\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.utils import first, set_determinism\n",
    "from monai.transforms import (\n",
    "    Compose, \n",
    "    RandRotate90, \n",
    "    RandFlip, \n",
    "    OneOf, \n",
    "    RandAdjustContrast, \n",
    "    RandGaussianNoise,  # MONAI does not directly support RandomBrightness, using Gaussian noise as an alternative\n",
    "    RandGaussianSharpen,  # MONAI does not have a direct match for RandomContrast, using Gaussian sharpen as an alternative\n",
    "    RandShiftIntensity,  # Alternative for adjusting brightness\n",
    "    Resize,\n",
    "    NormalizeIntensity\n",
    ")\n",
    "\n",
    "# Data loading code\n",
    "img_ids = glob(os.path.join('inputs', config['dataset'], 'images', '*' + config['img_ext']))\n",
    "img_ids = [os.path.splitext(os.path.basename(p))[0] for p in img_ids]\n",
    "\n",
    "train_img_ids, val_img_ids = train_test_split(img_ids, test_size=0.2, random_state=41)\n",
    "\n",
    "\n",
    "train_transform = Compose([\n",
    "    RandRotate90(prob=0.5, spatial_axes=(0, 1)),  # Randomly rotates the images by 90 degrees with a default probability of 0.5\n",
    "    RandFlip(prob=0.5, spatial_axis=0),  # Randomly flips the image along a given axis\n",
    "    OneOf([  # Applies one of the transforms\n",
    "        RandShiftIntensity(offsets=0.1, prob=1.0),  # Randomly shifts intensity for brightness adjustment\n",
    "        RandAdjustContrast(prob=1.0),  # Randomly changes contrast\n",
    "        RandGaussianSharpen(prob=1.0),  # Randomly sharpens the image\n",
    "    ]),\n",
    "    Resize(spatial_size=(config['input_h'], config['input_w'])),  # Resize images to a specified size\n",
    "    NormalizeIntensity(nonzero=True, channel_wise=True)  # Normalize pixel values with channel-wise option\n",
    "])\n",
    "\n",
    "val_transform = Compose([\n",
    "    Resize(spatial_size=(config['input_h'], config['input_w'])),  # Resize images to a specified size\n",
    "    NormalizeIntensity(nonzero=True, channel_wise=True)  # Normalize pixel values with channel-wise option\n",
    "])\n",
    "\n",
    "from analogainas.search_spaces.dataloaders.Nuclei_Dataset import Dataset as Dataset\n",
    "\n",
    "train_dataset = Dataset(\n",
    "    img_ids=train_img_ids,\n",
    "    img_dir=os.path.join('inputs', config['dataset'], 'images'),\n",
    "    mask_dir=os.path.join('inputs', config['dataset'], 'masks'),\n",
    "    img_ext=config['img_ext'],\n",
    "    mask_ext=config['mask_ext'],\n",
    "    num_classes=config['num_classes'],\n",
    "    transform=train_transform)\n",
    "\n",
    "val_dataset = Dataset(\n",
    "    img_ids=val_img_ids,\n",
    "    img_dir=os.path.join('inputs', config['dataset'], 'images'),\n",
    "    mask_dir=os.path.join('inputs', config['dataset'], 'masks'),\n",
    "    img_ext=config['img_ext'],\n",
    "    mask_ext=config['mask_ext'],\n",
    "    num_classes=config['num_classes'],\n",
    "    transform=val_transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config['batch_size'],\n",
    "    shuffle=True,\n",
    "    num_workers=config['num_workers'],\n",
    "    drop_last=True)\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=config['batch_size'],\n",
    "    shuffle=False,\n",
    "    num_workers=config['num_workers'],\n",
    "    drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Digital Training Loop\n",
    "\n",
    "log = OrderedDict([\n",
    "    ('epoch', []),\n",
    "    ('lr', []),\n",
    "    ('loss', []),\n",
    "    ('iou', []),\n",
    "    ('val_loss', []),\n",
    "    ('val_iou', []),\n",
    "])\n",
    "\n",
    "best_iou = 0\n",
    "trigger = 0\n",
    "for epoch in range(config['epochs']):\n",
    "    print('Epoch [%d/%d]' % (epoch, config['epochs']))\n",
    "\n",
    "    # train for one epoch\n",
    "    train_log = train(config, train_loader, model, criterion, optimizer)\n",
    "    # evaluate on validation set\n",
    "    val_log = validate(config, val_loader, model, criterion)\n",
    "\n",
    "    if config['scheduler'] == 'CosineAnnealingLR':\n",
    "        scheduler.step()\n",
    "    elif config['scheduler'] == 'ReduceLROnPlateau':\n",
    "        scheduler.step(val_log['loss'])\n",
    "\n",
    "    print('loss %.4f - iou %.4f - val_loss %.4f - val_iou %.4f'\n",
    "          % (train_log['loss'], train_log['iou'], val_log['loss'], val_log['iou']))\n",
    "\n",
    "    log['epoch'].append(epoch)\n",
    "    log['lr'].append(config['lr'])\n",
    "    log['loss'].append(train_log['loss'])\n",
    "    log['iou'].append(train_log['iou'])\n",
    "    log['val_loss'].append(val_log['loss'])\n",
    "    log['val_iou'].append(val_log['iou'])\n",
    "\n",
    "    pd.DataFrame(log).to_csv('models/%s/log.csv' %\n",
    "                              config['name'], index=False)\n",
    "\n",
    "    trigger += 1\n",
    "\n",
    "    if val_log['iou'] > best_iou:\n",
    "        torch.save(model.state_dict(), 'models/%s/model.pth' %\n",
    "                    config['name'])\n",
    "        best_iou = val_log['iou']\n",
    "        print(\"=> saved best model\")\n",
    "        trigger = 0\n",
    "\n",
    "    # early stopping\n",
    "    if config['early_stopping'] >= 0 and trigger >= config['early_stopping']:\n",
    "        print(\"=> early stopping\")\n",
    "        break\n",
    "\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from analogainas.search_spaces.resnet_macro_architecture import Network\n",
    "from analogainas.search_spaces.train import train_config_unet\n",
    "from analogainas.utils import *\n",
    "\n",
    "# Building the Optimal Sub-network\n",
    "model = Network(config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "supernetwork_nas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
